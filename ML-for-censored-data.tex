%\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax
\documentclass[acmtkdd]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
\acmVolume{1}
\acmNumber{1}
\acmArticle{1}
\acmYear{0000}
\acmMonth{1}

\usepackage{fullpage}
\usepackage{natbib}
%%% OMIT IMAGES - comment out to get them back %%%
%\usepackage[demo]{graphicx}
%\usepackage{graphicx}
%
%\usepackage{epsf,pstricks}
%\usepackage{authblk}
\usepackage{amsmath}
%\usepackage{amsbsy}
%\usepackage{amsfonts}
%\usepackage{xcolor}
%\usepackage{bbm}
%\usepackage{multirow}
%\usepackage{textcomp}
%\usepackage{caption}
%\usepackage{subcaption}
%\captionsetup{compatibility=false}
%\usepackage[%           % Fine in most cases
%			pdfpagelabels,hypertexnames=true,
%			plainpages=false,
%			naturalnames=false]{hyperref}
%\usepackage{color}
%\definecolor{darkblue}{rgb}{0,0.1,0.5}
%\hypersetup{colorlinks,
%			linkcolor=blue,
%			anchorcolor=blue,
%			citecolor=blue}
%%\def\pgfsysdriver{pgfsys-dvipdfm.def}
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
%\usepackage{algorithm}
%%\usepackage{algorithmic}
%\usepackage{algcompatible}
%\usepackage{eqparbox}
%\renewcommand{\algorithmiccomment}[1]{\hfill\eqparbox{COMMENT}{#1}}
%\algrenewcommand\algorithmicindent{1.0em}
\newcommand{\INDSTATE}[1]{\STATE\hspace{#1}}
\newcommand*{\h}{\hspace{5pt}}% for indentation
\newcommand*{\ha}{\hspace{1em}}% for indentation
\newcommand*{\hh}{\h\h}% double indentation
\newcommand{\argmin}{arg\,min}
\newcommand\mycomment[1]{\textcolor{red}{\sffamily [#1]}}
\newcommand\mbf[1]{\mathbf{#1}}
\newcommand\mrm[1]{\mathrm{#1}}
\newcommand\indicatorBig[1]{{\mathbb{I}}[#1]}
\newcommand\indicator[1]{{\mathbb{I}}(#1)}
%\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\uag}{\multirow{1}{*}{\textcolor{green}{\bf\Large\textuparrow}}}
\newcommand{\daG}{\multirow{1}{*}{\textcolor{green}{\bf\Large\textdownarrow}}}
\newcommand{\uar}{\multirow{1}{*}{\textcolor{red}{\bf\Large\textuparrow}}}
\newcommand{\dar}{\multirow{1}{*}{\textcolor{red}{\bf\Large\textdownarrow}}}
\newcommand{\mc}[1]{\multicolumn{2}{|c|}{#1}}
\newcommand{\vrt}[1]{\rotatebox{90}{\mbox{#1}}}
\newcommand{\bbeta}{\mathbf{\beta}}
\newcommand{\footremember}[2]{%
   \footnote{#2}
    \newcounter{#1}
    \setcounter{#1}{\value{footnote}}%
}
\newcommand{\footrecall}[1]{%
    \footnotemark[\value{#1}]%
}



\begin{document}

% Page heads
\markboth{D. Vock et al.}{ML with censored time-to-event data}

% Title portion
\title{Adapting machine learning techniques to censored time-to-event data: a general approach using inverse proability of censoring weighting}
\author{DAVID M. VOCK
\affil{Division of Biostatistics, School of Public Health, University of Minnesota}
JULIAN WOLFSON
\affil{Division of Biostatistics, School of Public Health, University of Minnesota}
SUNAYAN BANDYOPADHYAY
\affil{Department of Computer Science, University of Minnesota}
GEDIMINAS ADOMAVICIUS
\affil{Department of Information and Decision Sciences, Carlson School of Management, University of Minnesota}
PAUL E. JOHNSON
\affil{Department of Information and Decision Sciences, Carlson School of Management, University of Minnesota}
GABRIELA VAZQUEZ-BENITEZ
\affil{HealthPartners Institute for Education and Research}
PATRICK J. O'CONNOR
\affil{HealthPartners Institute for Education and Research}}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block (below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he is
%       currently affiliated with NASA.

\begin{abstract}
Models for predicting the probability of experiencing various health outcomes over a certain time frame (e.g., having a heart attack in the next 5 years) based on individual patient characteristics are important tools for managing patient care. Because electronic health data (EHD) from health care systems provide access to large amounts of individual-level data from contemporaneous patient populations, they are appealing sources of training data for building risk prediction models. Machine learning approaches to estimate risk are attractive because of their ability to capture complex relationships between individual characteristics and health outcomes, thereby allowing the population to be partitioned into distinct subgroups based on their risk. However, since EHD are derived by extracting information from administrative databases, some fraction of subjects will not be under observation for the entire time frame over which one wants to make predictions; this loss to follow-up is often due to disenrollment from the health system. For subjects without complete follow-up, the event status is unknown, and in statistical terms the event time is said to be right-censored. While there is a well-developed statistical literature on regression models which account for right-censored data, most machine learning approaches to the problem have been relatively {\em{ad hoc}}, for example, discarding the censored observations or treating them as non-events. In this paper, we present a rigorous, general-purpose approach to account for right-censored outcomes using the inverse probability probability of censoring weighting (IPCW). We illustrate how IPCW can easily be incorporated into existing machine learning algorithms, and show that our approach leads to better predictive performance than {\em{ad hoc}} approaches. Our techniques are motivated by and illustrated on the problem of predicting the 5-year risk of experiencing a cardiovascular event, using EHD from a large U.S. Midwestern health care system.
\end{abstract}

\category{G.3}{Probability and Statistics}{Survival analysis}

\terms{Algorithms, Theory}

\keywords{Machine learning, censored data, electronic health data, survival analysis, inverse probability of censoring weights, risk prediction, medical decision support.}

\acmformat{David M. Vock, Julian Wolfson, Sunayan Bandyopadhyay, Gediminas Adomavicius, Paul E. Johnson, Gabriela Vazquez-Benitez, and Patrick J. O'Connor, 2010. Adapting machine learning techniques to censored time-to-event data: a general approach.}
% At a minimum you need to supply the author names, year and a title.
% IMPORTANT:
% Full first names whenever they are known, surname last, followed by a period.
% In the case of two authors, 'and' is placed between them.
% In the case of three or more authors, the serial comma is used, that is, all author names
% except the last one but including the penultimate author's name are followed by a comma,
% and then 'and' is placed before the final author's name.
% If only first and middle initials are known, then each initial
% is followed by a period and they are separated by a space.
% The remaining information (journal title, volume, article number, date, etc.) is 'auto-generated'.

\begin{bottomstuff}
This work was partially supported by NHLBI grant R01HL102144-01 and AHRQ grant R21HS017622-01.

Author's addresses: TO FILL IN.
\end{bottomstuff}

\maketitle

\section{Introduction}

Predictions of the ``personalized'' risk of a patient experiencing various health outcomes (e.g., heart attack, stroke, diabetes, etc.) are critical tools in clinical practice. Risk prediction and stratification help clinicians to optimize resource allocation, to develop appropriate intervention strategies for those at high risk for experience an adverse health outcome, and to motivate patients to remain adherent to that strategy. Additionally, risk prediction tools, especially when incorporated into primary care, can help raise awareness of the burden of various diseases and the risk factors associated with them. Given the importance of risk prediction and stratification in the clinical setting, there is currently great interest in developing methods to estimate the ``personalized'' risk of a patient experiencing various health outcomes.


For many diseases, there is no shortage of risk prediction algorithms. Considering cardiovascular disease and related outcomes (e.g., heart attack, stroke), the application area which motivates our work here, recent systematic reviews have found over 100 risk models produced between 1999 and 2009 alone \citep{Cooney_2009,Cooney_2010,matheny2011systematic} including the well-known Framingham \citep{DAgostino_2008}, SCORE \citep{Conroy_2003}, ASSIGN-SCORE \citep{Woodward_2007}, QRISK1 \citep{HippisleyCox_2007,HippisleyCox_2008}, QRISK2 \citep{HippisleyCox_2008a}, PROCAM \citep{Assmann_2002}, WHO/ISH, and Reynolds Risk Score \citep{Ridker_2007,Ridker_2008}. Most well-known risk prediction models, including those surveyed above, have been estimated using data from carefully selected epidemiological cohorts: for example, the Framingham risk score is trained on a data set that excludes patients that have had a previous CV event, represents a predominantly Caucasian population, and includes patients from the late 1960s \citep{DAgostino_2008}. As a result of estimating the risk of CV events using data from these homogeneous cohorts, existing risk models are likely to only give accurate predictions for patients who are well represented in the training data sets.

The increasing availability of electronic health data (EHD) represent a key opportunity to improve risk prediction models.  EHD, which consist of electronic medical records (EMRs), insurance claims data, and mortality data obtained from the state government, are increasingly available within the context of large health care systems and capture the characteristics of a heterogeneous population receiving care in a contemporary clinical setting. EHD databases typically include records on hundreds of thousands to millions of individual patients; therefore, a risk prediction model constructed from EHD has the potential to yield more accurate and generalizable risk predictions than a model built using cohort data because even relatively specific sub-populations (e.g., patients with multiple comorbidities) are likely to be well-represented in such a large database.


The scale and complexity of EHD data provide an excellent opportunity to develop more accurate risk models using modern machine learning techniques \citep{Wu2010, song2004comparison,colombet2000models}. However, EHD provides a major challenge to building risk prediction models using ``off-the-shelf'' machine learning tools: in many datasets derived from EHD, a large fraction of subjects do not have enough follow-up data available to ascertain whether or not they experienced the event of interest over a given time period (e.g., a CV event over 5 years). In the language of statistical survival analysis, such subjects are said to be \emph{right-censored}. There are several regression-based methods that handle right-censored data including the Cox proportional hazards model \citep{Cox_1972} and, to a lesser extent, the accelerated failure time model \citep{Buckley_1979}.  However, the proportional hazards (accelerated failure time) model assumes that the risk factors have a linear relationship with the log hazard (log time) of experiencing a CV event which can be limiting. If the analyst has {\em{a priori}} knowledge that this relationship is non-linear or differs in certain sub-groups, he or she may include non-linear transformations of  predictors or interactions between predictors, but this is often based on trial and error \citep{Kattan_1998}, and a more flexible, machine learning approach is often warrented.

Fully supervised machine learning methods typically assume that the event indicator is known for all subjects, while in our setting the event indicator is undetermined for subjects who are censored, i.e., who do not experience an event and are not followed for the full time period over which one wants to make predictions (e.g., 5 years). Existing approaches to dealing with this issue, such as discarding censored observations (see, e.g., \cite{Larranaga_1997,Sierra_1998,Blanco_2005}) or treating them as zeroes (non-events), are known to induce bias in the estimation of class probabilities \citep{Kattan_1998}, making typical fully supervised classification approaches unsuitable. For example, \cite{Stajduhar_2009} demonstrated the impact of unaccounted-for censoring on the construction and performance of Bayesian networks. Semi-supervised approaches are also generally not applicable since the labeled (non-censored) and unlabeled (censored) observations are not samples from the same underlying population, and censored observations are not truly `unlabeled' since they carry useful partial information about the outcome.

There has been increasing interest in adapting machine learning techniques to censored, time-to-event data.  \citet{ishwaran2008random} describe the random survival forests technique, and \citet{Lucas_2004} discuss the application of Bayesian networks to right-censored data. \citet{Zupan_2000} and \citet{Stajduhar_2010} have proposed approaches in which censored observations are repeated twice in the dataset, one as experiencing the event and one event-free. Each of these observations are assigned a weight based on the marginal probability of experiencing an event between the censoring time and $\tau$, the time the event status will be assessed. This approach, although intuitive, is provably biased and inconsistent because the method to weight each of the replicated observations is based on the marginal probability and does not properly account for the relationship between the features and outcome.  \citet{Stajduhar_2012} adopt a more principled likelihood-based approach to imputing event times, but their imputation technique may perform poorly if the assumed parametric distribution of event times is incorrect. Other approaches, including replacing the time-to-event with the martingale from the null model,  have been proposed to handle censored data in other machine learning methods including support vector regression, recursive partitioning, and multiple adaptive regression splines \citep{Therneau_1990,Kattan_1998,Kattan_2003}. These approaches require that the technique to mine the data permit a continuous outcome which restricts the class of machine learning techniques that can be applied.

In this paper, we propose a general-purpose technique for mining right-censored time-to-event data using inverse probability of censoring weights (IPCW). The technique properly accounts for censoring and can be easily integrated into many existing class probability estimation procedures, allowing sophisticated (possibly ensemble-based) machine learning tools for censored data to be created with minimal programming effort. The advantage of our proposed approach is that it may be incorporated in any software package that allows for so-called ``observation weights.'' We begin by introducing the IPCW method and briefly reviewing its statistical properties. We then illustrate how IPCW can be applied to create ``censoring-aware'' versions of several popular prediction methods: logistic regression, generalized additive models, Bayesian networks, binary decision trees, and k-nearest neighbors.  We also argue that traditional evaluation metrics for assessing model calibration and classification accuracy (e.g., AUC  and net reclassification improvement) may be misleading in the presence of censoring, and describe alternatives which are more appropriate for use in selecting model tuning parameters. We conclude by applying IPCW machine learning methods to predict the occurrence of cardiovascular events from electronic health data collected by a large Midwest health maintenance organization.

\section{Inverse probability of censoring weighting}
\label{sect:IPCW}

\subsection{Notation}
Let $E$ be the indicator that an event or health outcome occurs prior to some fixed time $\tau$ (e.g., 5 years) once we start following a patient. Binary classification methods typically assume that $E$ is fully observed for all patients in the data set (e.g., whether or not a patient experienced a heart attack within 5 years of following them), but this is unlikely to be true when using information from contemporary EHD. In our application, once a patient leaves the health system or the study ends, their health state (i.e., features of the risk prediction model) and event history are no longer recorded in the EMR. If the patient's follow-up ends prior to time $\tau$, then their event status at $\tau$ is unknown and their event indicator is said to be \emph{right-censored}. To establish notation which is standard in the statistical literature, for individual $i$ define $T_i$ as the time between the beginning of the follow-up period and the event of interest, and define $C_i$ as the time between the beginning of the follow-up period and when the patient is lost to follow-up (e.g., in our context, disenrolls from the health plan or reaches the end of the data capture period without experiencing an event). We observe $V_i=\min(T_i,C_i)$ and $\delta_i=\indicator{T_i<C_i}$, the indicator for whether or not an event was observed.  If $\delta_i = 0$, the subject's event time is right-censored. We can only ascertain that an event occurred ($E_i = 1$) if $\delta_i=1$, or that an event did not occur ($E_i = 0$) if $\delta_i=0$ and $V_i > \tau$. In other words, the value of $E_i$ is only known if $\min (T_i,\tau) < C_i$. We will denote the set of features available on individual $i$ by $\mathbf{X_i}$; it is assumed that these features are fully observed at the beginning of the follow-up period and hence are not subject to censoring and do not vary over time.

\subsection{The IPCW method}
\label{sect:IPCWeights}

As mentioned earlier, one naive approach to handling the subjects for whom we cannot ascertain the value of $E$ would be to exclude them from our training data set or to set $E=0$, but both approaches would lead to biased estimators of the risk given features, $P(E=1 | \mathbf{X_i})$, and hence potentially poor classification performance.  Instead, we propose to adjust for right-censoring using an inverse probability of censoring weighting (IPCW) approach. In the IPCW approach, only those patients for whom we can determine $E$ contribute directly to the analysis, but they are reweighted to accurately ``represent'' the patients who were censored prior to $\tau$ and were, therefore, omitted from the analysis. For example, patients that have a longer time to event are more likely to be censored and hence receive larger weights. Recalling that $\tau$ is the time period over which we wish to characterize whether or not a patient experiences an event, the general-purpose IPCW method proceeds as follows:

\begin{enumerate}
\item Using the training data, estimate $G(\tau)=P(C_i>\tau)$, the probability that the censoring time is greater than $\tau$, using the Kaplan-Meier estimator of the survival distribution (i.e., 1 minus the cumulative distribution function) of the censoring times \citep{Kalbfleisch_2002}.  The Kaplan-Meier estimator of the censoring process is given by

\begin{equation} \label{eq:KMdef}
\hat{G}(\tau)=\prod_{j:t_{j}<\tau}\left( \frac{n_{j}-d^*_{j}}{n_{j}} \right )
\end{equation}
where $d^*_{j}$ is the number of subjects who were censored at time $t_{j}$ and $n_{j}$ is the number of subjects ``at risk'' for censoring (i.e., not previously censored or experiencing an event) at time $t_{j}$. Unlike other {\em{ad hoc}} approaches to handling censored observations, the Kaplan-Meier estimator is a consistent estimator of $G$ \citep{Kalbfleisch_2002} provided that the censoring times are independent of the event times. We note that, for IPCW, Kaplan-Meier is applied to estimate the distribution of \emph{censoring times}, whereas it is much more commonly used to estimate the distribution of \emph{event times}. Standard software functions for computing the Kaplan-Meier estimator of events times can be used to estimate $G$ by setting the ``event'' indicators to $\delta^*_i = 1 - \delta_i$.

\item For each patient $i$ in the training set, define a weight
\begin{equation} \label{eq:IPCWdef}
\omega_i = \left\{ \begin{array}{cl}
\frac{1}{ \hat{G}( \min(V_i, \tau) ) } & \text{if } \min (T_i,\tau) <C_j	 \\
0 &  \text{otherwise}
	\end{array} \right.
\end{equation}

\item Apply an existing prediction method to a weighted version of the training set, where each member $i$ of the training set is assigned weight $\omega_i$.
\end{enumerate}

Step 3 is left purposefully vague, as the incorporation of training set weights will vary according to the prediction technique used. However, we note that conceptually it is as if we created a new dataset in which each subject appeared in the dataset $\omega_i$ times. As $\omega_i$ may not be an integer, we cannot do this exactly in practice, but this is helpful from a conceptual perspective. In the next section, we illustrate how inverse probability weighted instances can be used to train three popular risk prediction methods.


%%%
\section{Risk prediction evaluation metrics for censored data} \label{sce:performanceMetrics}

A key feature of many machine learning techniques for risk prediction is that they can be ``tuned'' by adjusting parameters to optimize a performance metric, e.g., the misclassification rate. For the same reasons described above that failing to account for censoring yields biased parameter estimates, the usual performance metrics applied to risk prediction problems can be misleading when outcomes are subject to censoring. In this section, we present modifications of standard calibration (goodness-of-fit test statistic) and discrimination (concordance index and net reclassification improvement) metrics which properly account for censored data and allow model performance to be assessed more accurately. In the Section \ref{sect:examples}, we describe how these modified metrics can be used to select tuning parameter values for IPC-weighted versions of canonical machine learning techniques.

\subsection{Calibration}\label{sec:calibration}

For standard class probability estimation problems, calibration is commonly assessed by ranking the predicted class probabilities for the test set, binning the ranked predictions (e.g., by decile or clinically relevant cut points), and comparing the mean predicted class probability in each bin to the empirical class probability of the instances in that bin. When the outcome $E_i$ is not fully observed on all subjects, the empirical class probability of all the observation within a bin cannot be found using the sample mean.  An alternative adapted to a censored data setting estimates the probability of experiencing an event prior to time $\tau$ within each bin using the Kaplan-Meier estimator. A calibration statistic can be formed:
\begin{equation}
K = \sum_{j=1}^B \frac{ ( \bar p_j - \hat{p}^{KM}_j )^2 }{ var(\hat{p}_{j}^{KM})}
\end{equation}
\begin{equation}\label{greenwood_formula}
var(\hat{p}_{j}^{KM}) = var\{\hat{S}_j(\tau)\} = \hat{S}_j(\tau)^2\sum_{t_i<\tau}\frac{d_{ij}}{n_{ij} - d_{ij}}
\end{equation}
where $B$ is the number of bins, $\bar p_j$ is the average of predicted probabilities in bin $j$, $\hat{p}^{KM}_j$ is the Kaplan-Meier estimate of experiencing an event before $\tau$, $\hat{S}_{j}(\tau)$ is its corresponding survival rate (which equals $1-\hat{p}_j^{KM}$), $var\{\hat{p}_{j}^{KM}\}$ is the sampling variance of the Kalplan-Meier estimator calculated using Greenwood's formula \citep{greenwood1926report} applied to the data in bin $j$, $d_{kj}$ is the number of events occurring at time $t_k$ in bin $j$, and $n_{kj}$ are the number of people ``at risk'' for an event at time $t_k$ (i.e., not censored and not experiencing an event before time $t_i$). $K$ is analogous to the $\chi^2$ statistic with $B-2$ degrees of freedom for assessing the calibration of logistic models suggested by \cite{Hosmer_1980,Lemeshow_1982}. Calibration plots can be used compare predicted and Kaplan-Meier probabilities of experiencing an event before $\tau$ within bins defined by ranges of predicted probabilities.

\subsection{Concordance index}\label{sec:cIndex}

The area under the ROC curve (AUC) is a widely used summary measure of predictive model performance. When the outcome is fully observed on all subjects, tt is equivalent to the concordance index (C-index), the probability of correctly ordering the outcomes for a randomly chosen pair of subjects whose predicted risks are different. Standard techniques for estimating the AUC/C-index are potentially biased when data are censored. However, as described in \citet{Harrell}, The C-index can be adapted for censoring by considering the concordance of survival outcomes versus predicted survival probability among pairs of subjects whose survival outcomes can be ordered, i.e., among pairs where both subjects are observed to experience a CV event, or one subject is observed to experience a CV event before the other subject is censored. Pairs in which both subjects are censored or in which the censoring time of one precedes the failure of the other do not contribute to this metric. Let $\hat{P}_{E_i|\mbf{X}_i}(e_i=1|\mbf{x}_i)$ be the estimated probability that the $i^{th}$ subject experiences an event within $\tau$ years. Then the C-index adapted for censoring is given by
\begin{equation}
C_{cens}(\tau) = \frac{ \sum_{i \neq j}  \delta_i \indicatorBig{V_i<V_j}\indicatorBig{ \hat{P}_{E_i|\mbf{X}_i}(e_i=1|\mbf{x}_i) <\hat{P}_{E_j|\mbf{X}_j}(e_j=1|\mbf{x}_j) }}{ \sum_{i \neq j} \delta_i \indicatorBig{V_i<V_j} },
\end{equation}
where $\indicatorBig{\cdot}$ is the indicator function.

Note that the only pairs which contribute to $C_{cens}(\tau)$ are those where one subject experiences an event prior to $\tau$ and the other is known not to have experienced an event before the first subject.

\subsection{Net Reclassification Improvement}\label{sec:NRI}

The C-index may be inadequate to distinguish between models that differ in relatively modest but clinically important ways. One proposed alternative is the Net Reclassification Improvement (NRI) \citep{pencina2008evaluating}. The NRI compares the number of ``wins'' for two models among discordant predictions. It has been argued that NRI is a particularly relevant measure of comparison between models in the clinical domain, where it is often more important to discriminate between lower and higher risk patients than to estimate their risk precisely. The NRI is computed by cross-tabulating predictions from two different models with table cells defined by clinically meaningful cardiovascular risk categories or bins, then comparing the agreement of discordant predictions with actual event status. Formally, the NRI for comparing prediction models $M_1$ and $M_2$ using fully observed (i.e., not censored) binary event data is given by:
\begin{equation}\label{eq:NRI}
\mathrm{NRI}(M_1,M_2) = \frac{E_{M_1}^{\uparrow} - E_{M_2}^{\uparrow}}{n_E} + \frac{ \bar{E}_{M_1}^{\downarrow} - \bar{E}_{M_2}^{\downarrow}}{ n_{\bar{E}}}
\end{equation}

Here $E_{M_1}^{\uparrow}$ is the number of individuals in the test set who experienced events and were placed in a higher risk category by $M_1$ than $M_2$ (i.e., a number of ``wins'' for $M_1$ over $M_2$ among patients who had events), and the opposite change in risk categorization yields $E_{M_2}^{\uparrow}$). Similarly, $\bar{E}_{M_1}^{\downarrow}$ and $\bar{E}_{M_2}^{\downarrow}$ count the number of individuals who did not experience an event and were ``down-classified'' by $M_1$ and $M_2$, respectively (i.e., ``wins'' among patients who did not have events). $n_E$ and $n_{\bar E}$ are the total number of patients with events and non-events, respectively.  A positive $\mathrm{NRI}(M_1,M_2)$ means better reclassification performance for $M_1$, while a negative $\mathrm{NRI}(M_1,M_2)$ favors $M_2$.

Omitting subjects with less than $\tau$ years of follow-up (or treating them as non-events) will result in biased estimates of the NRI.  To evaluate risk reclassification on our test data which are subject to censoring, a ``censoring-adjusted'' NRI (cNRI) due to \cite{Pencina_2011} takes the form:

\begin{equation}
\mathrm{cNRI}(M_1,M_2) = \frac{E_{M_1}^{*,\uparrow} - E_{M_2}^{*,\uparrow}}{n^*_E} + \frac{ \bar{E}_{M_1}^{*,\downarrow} - \bar{E}_{M_2}^{*,\downarrow}}{ n^*_{\bar{E}}}
\label{eq:cNRI}
\end{equation}

where $E_{M_1}^{*,\uparrow}, E_{M_1}^{*,\downarrow}, E_{M_2}^{*,\uparrow}, E_{M_2}^{*,\downarrow}, n^*_E$ and $n^*_{\bar E}$ are analogous to the quantities in \eqref{eq:NRI}, but correspond to expected number of subjects in each category, with the expectations computed using the Kaplan-Meier estimator to account for censoring.


%%%
\section{Applying IPCW with existing machine learning techniques: 3 examples}
\label{sect:examples}

%%
\subsection{(Generalized Additive) Logistic regression}

Logistic regression is a simple and popular technique for modeling binary or binomial data. The goal is a find a linear combination of features to approximate the log-odds, i.e.,
\begin{equation}
\log \left( \frac{ p_i(\mathbf{x}) }{ 1 - p_i(\mathbf{x}) } \right) = \beta_0 + \sum_{j=1}^p \beta_j x_j
\end{equation}
where $p_i(\mathbf{x}) = P(E_i=1 | \mathbf{X_i} = \mathbf{x})$ for the vector of features $\mathbf{X_i}$. The features may take any form, but in risk prediction the ``base'' model often includes the so-called main effects of each risk factor, i.e., the value of the risk factor itself. Given features $\mathbf{X}$ and a corresponding vector of event indicators $\mathbf{E}$, the logistic regression log-likelihood takes the form
\begin{equation}
\ell(\bbeta; \mathbf{X},\mathbf{E}) = \sum_{i=1}^n \left[ E_i \log p_i(\mathbf{x}) + (1-E_i) \log( 1 - p_i(\mathbf{x}) ) \right],
\label{eq:logreglike}
\end{equation}
where $n$ is the number of observation in the training set, and this log-likelihood can be maximized using a number of techniques, the most common of which is iteratively reweighted least squares [REF]. The solution of $\partial \ell / \partial \bbeta = \mathbf{0}$ is the unique maximum likelihood estimator of $\bbeta$.

Logistic regression using the ``base'' model with only the main (linear) effects of various risk factors is relatively unlikely to produce a well-fitting model because the log odds of experiencing the event is unlikely to be linearly related to the features.  Enlarging the feature set by considering a basis expansion of the continuous features may improve prediction. The most intuitive expansion is the polynomial expansion where higher order moments of the $j^{th}$ feature $X_{ij}$, e.g. $X_{ij}^2, X_{ij}^3$ are included in the model. However, this particular expansion can be unstable so typically restriced cubic smoothing splines, B-splines, or thin-plate regression splines are frequently used in practice. Because expanding the feature space involves estimating many more parameters than just the including the linear risk factors, we typically penalize the ``wiggliness'' of the  linear predictor. If $\mbf{z}_j$ is the basis expansion of the $j^{th}$ feature and $\mbf{\beta}_j$ is vector of dimension equal to the dimension of $\mbf{z}_j$, then the generalized additive logistic model assumes that

\begin{equation}
\log \left( \frac{ p_i(\mathbf{x}) }{ 1 - p_i(\mathbf{x}) } \right) = \beta_0 + \sum_{j=1}^p {\mbf{\beta}}_j^T \mbf{z}_j,
\end{equation}

and the parameters are estimated by maximizing the penalized log likelihood. The tuning parameters $\lambda_j$ are typically selected to minimize the unbiased risk estimator (UBRE) which in the case of logistic regression is proportional to the Akaike Information Criterion \citep{Akaike1974} given by $AIC = 2 k - 2 \ell$, where $\ell$ is the (log-)likelihood given in \eqref{eq:logreglike}.

\begin{equation}
\ell(\bbeta; \mathbf{X},\mathbf{E}) = \sum_{i=1}^n \left[ E_i \log p_i(\mathbf{x}) + (1-E_i) \log( 1 - p_i(\mathbf{x}) ) \right] -  \sum_{j=1}^p \lambda_j \mbf{\beta_j}^T S_j \mbf{\beta_j},
\label{eq:logreglike_gam}
\end{equation}
where $\mbf{\beta} = (\beta_0, \mbf{\beta}_1^T, \ldots, \mbf{\beta}_p^T)^T$, $S$ is an appropriately chosen smoothing matrix, and $\lambda_j$ is a tuning parameter. The tuning parameter is

\subsubsection{IPC-weighted logistic regression}

IPC-weighted logistic regression maximizes the \emph{weighted} log-likelihood
\begin{equation}
\ell^\omega(\bbeta; \mathbf{X},\mathbf{E}) = \sum_{i=1}^n \omega_i \left[  E_i \log p_i(\mathbf{x}) + (1- E_i) \log( 1 - p_i(\mathbf{x}) ) \right]
\label{eq:logreglike-weighted}
\end{equation}

The weights are easily incorporated in standard statistical software. For example, in MATLAB IPC weights can be used in the \texttt{weights} argument of the \texttt{glmfit} function. In R \citep{}, the \texttt{weights} argument of the \texttt{glm} command can be used, or IPC weights can be specified as sampling weights in \texttt{svyglm} from the \texttt{survey} package \citep{}. The former approach in R will generate a warning as the \texttt{weights} argument is designed to specify the number of binomial trials corresponding to each row of the dataset, but the resulting likelihood differs from \eqref{eq:logreglike-weighted} by only a constant factor and hence estimation/prediction is identical to the \texttt{svyglm} approach which is explicitly designed to handle weights of this type.

Similarly, the IPC-weighted generalized additive logistic regression maximizes the following weighted penalized log-likelihood 

\begin{equation}
\ell^\omega(\bbeta; \mathbf{X},\mathbf{E}) = \sum_{i=1}^n \omega_i \left[  E_i \log p_i(\mathbf{x}) + (1- E_i) \log( 1 - p_i(\mathbf{x}) ) \right] -  \sum_{j=1}^p \lambda_j \mbf{\beta_j}^T S_j \mbf{\beta_j}.
\label{eq:logreglike-weighted_gam}
\end{equation}

As with standard logistic regression, standard statistical software which fits generalized additive models typically easily incorporates observation weights. For example, in R, the \texttt{weights} argument of the \texttt{gam} function in the \texttt{mgcv} package can be used. We note that the scores used to select the tuning parameters in the generalized additive model are also easily modified using IPC-weights to account right censoring. In particular, the weighted AIC becomes $AIC^\omega = 2k - 2 \ell^\omega$, with $\ell^\omega$ given in \eqref{eq:logreglike-weighted}.

As an alternative to estimating the parameters in the generalized additive logistic model using penalized maximum likelihood, we note \citet{Friedman2000a} showed that $F(\mathbf{x}) = \log \left( \frac{ p(\mathbf{x}) }{ 1 - p(\mathbf{x}) } \right)$ minimizes $E(e^{-y F(\mathbf{x})})$ and hence it is possible to derive boosting procedures (e.g., LogitBoost) which construct flexible additive predictive models by iteratively maximizing $\ell$. These approaches are easily generalized by maximizing $\ell^\omega$ at each step instead of $\ell$. Note that the IPC weights applied to the likelihood are distinct from the iteratively updated ``case weights'' used in the boosting algorithm to increase the influence of poorly-classified instances, and in practice the weights used in the algorithm will be a product of the two types of weights.

%%%
\subsection{Bayesian networks}
\label{sect:BN}

Bayesian networks are well-suited to handle the intricacies of risk prediction from complex health data. Compared to support vector machines or neural networks, Bayesian networks have a clear edge in interpretability, which is important to the end-users of these prediction models in the healthcare domain (e.g., physicians and clinical researchers). Because of their interpretability and their ability to aid in reasoning with uncertainty, Bayesian networks have been used extensively in biomedical applications (see \cite{Lucas_2004} for a review). In particular, they have been: used to aid in understanding of disease prognosis and clinical prediction \citep{Andreassen_1999,Verduijn_2007,Lipsky_2005,Sarkar_2013,Frances_2013,Lappenschaar_2013}; used to guide the selection of the appropriate treatment \citep{Lucas_2000,Kazmierska_2008,Smith_2009,Yet_2013,Velika_2014}; and implemented as part of clinical decision support systems \citep{Lucas_1998,Sesen_2013}.

%Let the features recorded on a patient be represented by a $p$-dimensional vector $\mathbf{X} = (X_1 \cdots X_p)$ where $X_i$ is the $i^\mathrm{th}$ risk factor (some of the factors could be missing for certain patients). Let $E=1$ indicate that an event (e.g., a CV event) occurred for a given patient within $\tau$ years of the beginning of the follow-up period, and $E=0$ indicate the absence of such an event in that time frame. Though our ultimate goal is to handle the case where $E$ is unknown for some patients, for now we assume that at least $\tau$ years of follow-up is available on each patient so that $E$ is fully observed.

% NOT NEEDED NOW with Notation Section: Let $E=1$ indicate that an event (e.g., a CV event) occurred for a given patient within $\tau$ years of the beginning of the follow-up period, and $E=0$ indicate the absence of such an event in that time frame. The target of estimation is $P_{E|\mathbf{X}}(e|\mathbf{x})$, the conditional probability that $E=e$ given the features $\mathbf{x}$ of a particular patient.

Using Bayes theorem, one can rewrite the conditional probability of an event, $P_{E|\mathbf{X}}(e|\mathbf{x})$,  as

\begin{equation} \label{eq:bayesBasic}
P_{E|\mathbf{X}}(e=1|\mathbf{x}) = \frac{P_{\mathbf{X}|E}(\mathbf{x}|e=1)P_E(e=1)}{\sum_{e\in\{0,1\}} P_{\mathbf{X}|E}(\mathbf{x}|e)P_E(e)},
\end{equation}
%so that the focus is shifted to estimation of the conditional density/probability $P_{\mathbf{X}|E}(\mathbf{x}|e)$ and the probability $P_E(e)$ for $e = 0,1$. To maintain notational brevity, we use $P_Y(y)$ to denote either the probability that the random variable $Y$ equals $y$ if $Y$ is discrete or the probability density of $Y$ evaluated at $y$ if $Y$ is a continuous random variable. Similarly, $P_{Y|Z}(y|z)$ is the conditional probability/density of the random variable $Y$ evaluated at $y$ given $Z=z$. In general, the dimensionality of the feature space $p$ may be too large to make joint modeling of $P_{\mathbf{X}|E}(\mathbf{x} | e=1)$ feasible.

Unlike the regression approaches described above which directly estimate $P_{E|\mathbf{X}}(e|\mathbf{x})$, focus is now shifted to estimation of the conditional density/probability $P_{\mathbf{X}|E}(\mathbf{x}|e)$ and the probability $P_E(e)$ for $e = 0,1$ To simplify the task of modeling $P_{\mathbf{X}|E}$, one can represent the joint distributions of $\mathbf{X}|E=e$ using a directed acyclic graph (DAG), i.e., a Bayesian network. The DAG encodes conditional independence relationships between variables, allowing the joint distribution to be decomposed into a product of individual terms conditioned on their parent variables \citep{stuart2003artificial}:

\begin{equation} \label{eq:DAGfactorsCond}
P_{\mbf{X}|E}(\mbf{x}|e) = \prod_{i=1}^p P_{X_i|\mrm{Pa}(X_i),E}\{x_i|\mrm{Pa}(x_i),e\}
\end{equation}
where $\mrm{Pa}(X_i)$ are the parents of $X_i$.

One advantage of the Bayesian network approach is that clinical knowledge can be used to suggest and refine DAG structures. While methods exist to infer the DAG structure from data, in our application we used parsimonious DAGs based on information from the medical literature as well as clinical judgment from the medical experts who collaborated on this research project.
%The DAG structure used in our predictive models is shown in Figure \ref{figure:BayesNet} (see the caption of the figure for a brief description) and is explained in greater detail in Section \ref{sec:data}. The graphical model in Figure \ref{figure:BayesNet} contains both continuous-valued nodes (which are elliptical in the figure) and discrete-valued nodes (which are rectangular). The network is therefore a \emph{hybrid Bayesian network}~\citep{murphy1998inference}.
%

When $E$ is observed on all subjects (i.e., there is no censoring), the maximum likelihood estimate of $P_E(e)$ is straightforward:
\begin{equation}\label{eq:PofE}
{\hat{P}}_E(e) = \frac{1}{n} \sum_{i=1}^n \indicatorBig{E_i=e}.
\end{equation}
%where $E_i$ is the CV event status for the $j^{th}$ person, $n$ is the number of people in the training set, and $\indicatorBig{\cdot}$ is the indicator function.

Several approaches have been proposed to modeling the terms $P_{X_i|\mrm{Pa}(X_i),E}\{x_i|\mrm{Pa}(x_i),e\}$. For example, a common strategy is to construct a regression model to link the values of continuous parent nodes of $X_i$ to the mean of $X_i$.  In applying Bayesian networks to our example data in Section \ref{sec:data}, we take a slightly different approach which is fully detailed in \citet{Bandyopadhyay2014}. For each feature $X_j$, we partition each group $\mbf{G}_j = \{X_j,\mathrm{Pa}(X_j)\}$ into $(\mathbf{Y}_j, \mathbf{Z}_j)$, where $\mbf{Y}_j$ represents the continuous risk factors and $\mbf{Z}_j$ the discrete risk factors. The joint distribution of $\mbf{G}_j$ given $E$ is $P_{\mbf{G}_j|E}(\mbf{g}_j|e)=P_{\mbf{Y}_j|\mbf{Z}_j,E}(\mbf{y}_j|\mbf{z}_j,e)\times P_{\mbf{Z}_j|E}(\mbf{z}_j|e)$, where $P_{\mbf{Z}_j|E}(\mbf{z}_j|e)$ is a discrete probability distribution. This distribution can be estimated by computing the proportion of observations in each unique state of $\mbf{Z}_j$ separately for $E=0$ or $E=1$, i.e.,
\begin{equation} \label{PofZ}
{\hat{P}}_{\mbf{Z}_j|E}(\mbf{z}_j|e) = \frac{{\hat{P}}_{\mbf{Z}_j,E}(\mbf{z}_j,e)}{ {\hat{P}}_E(e) } =
\frac{\frac{1}{n}\sum_{i=1}^n \indicatorBig {\mbf{Z}_{ij}=\mbf{z}_i, E_j=e}}{\frac{1}{n} \sum_{i=1}^n \indicatorBig { E_i=e}},
\end{equation}
where again $i$ indexes the subject. For the continuous components,  we model the density of $\mbf{Y}_i$ given $\mbf{Z}_i$ and $E$ as a mixture of $M$ multivariate normal densities  conditional on the levels of $\mathbf{Z}_j$:
\begin{equation}\label{eq:JoinDistComps2}
P_{\mbf{Y}_j|\mbf{Z}_j,E}(\mbf{y}_j|\mbf{z}_j,e)=\sum_{m=1}^M \rho_{j,m,z_j,e} \phi_{C_{\mbf{Y_j}}} \left\{ \Sigma^{-1/2}_{j,m,z_j,e} ( y_j - \mu_{j,m,z_j,e}) \right \},
\end{equation}
where $\phi_k(\cdot)$ is the density function of a $k$-variate standard normal random variable, $\mu_{j,m,z_j,e}$ and $\Sigma_{j,m,z_j,e}$ are the mean and variance matrix of $\mbf{Y}_j$ given $\mbf{Z}_j=\mbf{z}_j$ and $E=e$ for the $m^{th}$ multivariate normal density in the mixture, and $\rho_{j,m,z_j,e}$ are the mixing parameters where $\sum_{m=1}^M \rho_{j,m,z_j,e} = 1$.

For a fixed number of mixing components $M$ and given $E=e$ and $\mbf{Z}_j=\mbf{z}_j$, a standard expectation maximization (EM) algorithm \citep{dempster1977maximum} is used to solve for the maximum likelihood estimators of the mean, variance, and mixing parameters. In this well-studied Gaussian mixture problem, it is possible to derive explicit update formulas for both mixing and distributional parameters so that the ``E-step'' and ``M-step'' are performed simultaneously. In particular,
\begin{eqnarray} \label{eq:EMupdate}
\mu_{j,m,z_j,e}^{(\nu+1)} &=& \frac{ \sum_{i=1}^n p_{i,m}^{(\nu)} Y_{ij} } {\sum_{i=1}^n p_{i,m}^{(\nu)} } \\ \nonumber
\Sigma_{j,m,z_j,e}^{(\nu+1)} &=&\frac{ \sum_{i=1}^n p_{i,m}^{(\nu)} (Y_{ij}- \mu_{j,m,z_j,e}^{(\nu+1)})^T (Y_{ij}- \mu_{j,m,z_j,e}^{(\nu+1)})  } {\sum_{i=1}^n p_{i,m}^{(\nu)} } \\ \nonumber
\rho_{j,m,z_j,e}  &=& \frac{\sum_{i=1}^n p_{i,m}^{(\nu)} } {\sum_{i=1}^n \indicatorBig{\mbf{Z}_{ij}=z_j, E_i=e} },
\end{eqnarray}
where $p_{i,m}^{(\nu)} = \mathrm{E}_{\theta^{(\nu)}} (I_{j,m,z_j,e} |\mbf{Y}_{ij},E_i, \mbf{Z}_{ij}) \times \indicatorBig{\mbf{Z}_{ij}=z_j, E_i=e}$ and $I_{j,m,z_j,e}$ is the (latent) mixing indicator.  Additional details of the algorithm can be found in \citet{Bilmes_1998}. The choice of $M$ is discussed below.

\subsubsection{IPC-weighted Bayesian networks}

To fit the Bayesian network using IPCW, we make the following modifications:
\begin{enumerate}
\item Estimate IPC weights $\omega_j$ as described in Section \ref{sect:IPCWeights}.
\item Obtain the IPCW maximum likelihood estimator of $P_E(e)$ via
\begin{equation}
{\hat{P}}_E(e) = \frac{1}{n} \sum_{j=1}^n\indicatorBig{E_j=e} \omega_j.  \label{eq:PofE_IPCW} \\
\end{equation}
We note that this is equivalent to the Kaplan-Meier estimator of $P_E(e)$.
 
\item Obtain an IPCW maximum likelihood estimator of the distribution for the discrete variables $\mbf{Z_i}$ via
\begin{equation}
{\hat{P}}_{\mbf{Z}_i|E}(\mbf{z}_i|e) = \frac{\frac{1}{n}\sum_{j=1}^n \indicatorBig{\mbf{Z}_{ij}=\mbf{z}_i, E_j=e} \omega_j } {\frac{1}{n}\sum_{j=1}^n \indicatorBig{E_j=e}\omega_j} \label{eq:PofZ_IPCW},
\end{equation}
\item Obtain an IPCW estimator of the parameters in the mixture of Gaussian densities for the continuous variables $\mbf{Y_i}$ using a weighted EM algorithm. The contribution of maximum likelihood where the contribution of the $i^{th}$ subject to the likelihood is weighted by $\omega_i$. The update formulas for the parameter estimates previously given in Equation~\eqref{eq:EMupdate} become
\begin{eqnarray} \label{eq:EMupdate_IPCW}
\mu_{j,m,z_j,e}^{(\nu+1)} &=& \frac{ \sum_{i=1}^n \omega_i  p_{i,m}^{(\nu)} Y_{ij} } {\sum_{i=1}^n p_i^{(\nu)}\omega_i } \\ \nonumber
\Sigma_{j,m,z_j,e}^{(\nu+1)} &=&\frac{ \sum_{i=1}^n \omega_i  p_{i,m}^{(\nu)} (Y_{ij}- \mu_{j,m,z_j,e}^{(\nu+1)})^T (Y_{ij}- \mu_{j,m,z_j,e}^{(\nu+1)})  } {\sum_{i=1}^n p_i^{(\nu)}\omega_i } \\ \nonumber
\rho_{j,m,z_j,e}  &=& \frac{ \sum_{i=1}^n \omega_i  p_{i,m}^{(\nu)}} {\sum_{i=1}^n \indicatorBig{\mbf{Z}_{ij}=\mbf{z}_j,E_i=e} \omega_i},
\end{eqnarray}
where $p_{i,m}^{(\nu)} = E_{\theta^{(\nu)}} (I_{j,m,z_j,e} |\mbf{Y}_{ij},E_i, \mbf{Z}_{ij}) \times \indicatorBig{\mbf{Z}_{ij}=z_j, E_i=e}$ as before.
\end{enumerate}

Note that for subjects with $E=0$ (and $V > \tau$) the weights for all individuals are $1/\hat{G}(\tau)$ so the maximum likelihood estimators for $P_{\mbf{G}_j|E}(\mbf{g}_j|e=0)$ are the same as in the unweighted analysis. Further details of the weighted EM algorithm are provided in \citet{fraley2012mclust}. Many software packages implementing the EM algorithm (e.g., Matlab, R) allow weights to be provided as arguments to the EM function, making the IPCW Bayesian network approach straightforward to implement.

Tuning a Bayesian network for optimal performance may involve determining the network structure and/or controlling model complexity for a given structure. In the Bayesian network implementation we present below, we consider only a single network structure which is informed by discussions with our clinical colleagues; however, a set of feasible structures could easily be compared on a test set or via cross-validation using the calibration and reclassfication metrics described in Section \ref{sec:performanceMetrics}. Algorithms for learning graph structure (e.g., \citet{deCampos2011}) often use scoring criteria such as the BIC which we, as we show below, are easily adapted to the censored data setting.

In the above presentation of the Bayesian Network, a parameter $M$ controls the number of mixture components used to estimate the conditional distributions of continuous features.  One could consider $M$ to be a tunable parameter, and either select a single value or, as suggested in \citet{Bandyopadhyay2014}, use a model averaging procedure to combine results across multiple values of $M$.  \citet{Bandyopadhyay2014} determines the relative weights of each model using the Bayesian Information Criteria (BIC). In settings without censoring, $BIC = -2 \log(L) + k \log(n)$ where $l$ is the likelihood evaluated at the maximum likelihood parameter estimates, $n$ is the number of subjects in the training set, and $k$ the number of (free) parameters in the model. Since IPC weights are incorporated directly into the likelihood, it is straightforward to construct an IPC-weighted BIC appropriate for censored-data settings via $BIC^\omega = -2 \log(L^\omega) + k \log(\sum_i \omega_i)$, where$L^\omega$ is the IPC-weighted likelihood given in Equation (15) of \citet{Bandyopadhyay2014}.

%%
\subsection{Decision trees}

[ LITERATURE REVIEW ON DECISION TREES ]
To grow the tree, CART \citep{Breiman1984} uses the decrease in Gini impurity to determine which covariate and a what level to split a node. The change in Gini impurity for a splitting rule is given by
\[
\Delta I_G(S) = \hat p(S) ( 1- \hat p(S)) - \frac{1}{N_S} \left[ N_{S_L} \hat p(S_L) (1 - \hat p(S_L)) + N_{S_R} \hat p(S_R)( 1- \hat p(S_R)) \right]
\]
where $\hat p(S), \hat p(S_L),$ and $\hat p(S_R)$ are respectively the sample proportion of outcomes in a node $S$, and the left-hand and right-hand children for the particular splitting rule, $N_{S_L}$, and $N_{S_R}$ are the number of instances in each child, and $N_S = N_{S_L} + N_{S_R}$. In the unweighted case, the sample proportions are computed in the usual way (the nonparametric maximum likelihood estimators), i.e.:
\[
\hat p(S) = \frac{1}{N_S} \sum_{i \in S} E_i , \ \ \   \hat p(S_L) = \frac{1}{N_{S_L}} \sum_{i \in S_L} E_i , \ \ \ \hat p(S_R) = \frac{1}{N_{S_R}} \sum_{i \in S_R} E_i
\]
where $E_i$ is the binary event indicator. 

Once the structure of the tree has been determined though a series of binary splits, if we let $Z_{i}=j$ if the $i^{th}$ subject is in the $j^{th}$ terminal node, we can estimate $P_{E|Z}(e|z)$ using the standard (non-parametric) maximum likelihood estimator

\begin{equation*}
\hat{P}_{E|Z}(e|z) = \frac{\frac{1}{n}\sum_{i=1}^n \indicatorBig {E_i=e, Z_i=z}}{\frac{1}{n} \sum_{i=1}^n \indicatorBig { Z_i=z}}
\end{equation*}  

\subsubsection{IPC-weighted decision trees}

It is straightforward to extend decision trees to incorporate IPC weighting: individual cases in the training set are assigned weights $\omega_i$ as described above, and the $\omega_i$ are used as ``case weights'' in the decision tree algorithm. With IPC weighting, we simply calculate a weighted decrease in Gini impurity,
\[
\Delta I^\omega_G(S) = \hat p^\omega(S) ( 1- \hat p^\omega(S)) - \frac{1}{N^\omega_S} \left[ N^\omega_{S_L} \hat p^\omega(S_L) (1 - \hat p^\omega(S_L)) + N^\omega_{S_R} \hat p^\omega(S_R)( 1- \hat p^\omega(S_R)) \right]
\]
where
\[
N^\omega_S = \sum_{i \in S} \omega_i,
\ \ \ N^\omega_{S_L} = \sum_{i \in S_L} \omega_i,
\ \ \ N^\omega_{S_R} = \sum_{i \in S_R} \omega_i,
\]
and
\[
\hat p^\omega(S) = \frac{ \sum_{i \in S} \omega_i E_i }{ N^\omega_S },
\ \ \ \hat p^\omega(S_L) = \frac{ \sum_{i \in S_L} \omega_i E_i }{ N^\omega_{S_L} },
\ \ \ \hat p^\omega(S_R) = \frac{ \sum_{i \in S_R} \omega_i E_i }{ N^\omega_{S_R} },
\]

The identical approach can be applied to estimate a weighted version of the information gain metric which forms the basis of the C4.5 and C5.0 decision tree algorithms:
\[
\Delta I^\omega_E(S) =  \hat I^\omega_E(S) - \frac{1}{N^\omega_S} [ N^\omega_{S_L} \hat I_E^\omega(S_L) + N^\omega_{S_R} \hat I_E^\omega(S_R) ]
\]
where
\[
\hat I^\omega_E(\cdot) = - \hat p^\omega(\cdot) \log \hat p^\omega(\cdot) - (1 - \hat p^\omega(\cdot)) \log(1 - \hat p^\omega(\cdot))
\]

Again, once the structure of the tree has been determined, we estimate the probability of an event in the terminal nodes by taking the weighted (non-parametric) maximum likelihood estimator

\begin{equation*}
\hat{P}_{E|Z}(e|z) = \frac{\frac{1}{n}\sum_{i=1}^n \indicatorBig {E_i=e, Z_i=z}\omega_i}{\frac{1}{n} \sum_{i=1}^n \indicatorBig { Z_i=z}\omega_i}
\end{equation*}  

Because of their flexibility, regression trees often overfit training data. Many overfitting avoidance techniques have been proposed, with most involving a tuning parameter which restricts the complexity of the tree. One strategy consists of setting a lower limit $m$ on the number of individuals assigned to a terminal node; in our notation above, the node $S$ would not be split according to a given rule unless $\min(N_{S_L}, N_{S_R}) \geq m$. This strategy is easily generalized to the case with censoring by requiring that $\min(N^\omega_{S_L}, N^\omega_{S_R}) \geq m$; however we note that $N_{S_L} \approx N^\omega_{S_L}$ and $N_{S_R} \approx N^\omega_{S_R}$ as the expected value of the weights is one, so in practice we often control the size of the tree based on $\min(N_{S_L}, N_{S_R})$. Another approach requires that the information gain exceed a certain threshold $\theta$, e.g., $\Delta I_E(S) \geq \theta$. Substituting $\Delta I^\omega_E(S)$ for $\Delta I_E(S)$ (and similarly $\Delta I^\omega_G(S)$ for $\Delta I_G(S)$) allows the same rule to be used in the censored data setting. Final tuning parameter values may be chosen by cross-validation, where the cross-validated criterion to optimize could involve a measure of calibration, reclassification (C-index/NRI), or a combination of both.

%%%

\section{Statistical validity of IPCW}

The IPCW approach to handling censored event times assumes that the censoring time $C$ is independent of the event time $T$ and all features $\mbf{X}$. In our study, most patients are censored due to the end of the study or because they disenroll from the HMO due to a change in employment, reasons unrelated to their health status (i.e., $\mbf{X}$ and $T$).

We briefly argue why inverse probability of censoring weighting appropriately handles censoring and leads to well-calibrated risk prediction across a variety of machine learning techniques. Fundamentally, to obtain estimates of class probability (across all the machine learning scenarios considered in this manuscript) if there were no censoring, we estimate $P(E|\mbf{Z}_i)$ using some form of maximum likelihood estimators (e.g., non-parametric, penalized, etc.) where $\mbf{Z}_i$ is some function of the observed features $\mbf{X}_i$. For example, in logistic regression, we parameterize the log odds (i.e. $\log [P(E|\mbf{Z}_i)/ \{1-P(E|\mbf{Z}_i)\}]$) in terms of a linear combination of features $\mbf{X}_i$ and any non-linear and interaction terms specified {\em{a priori}} and estimate the coefficients in the linear combination using maximum likelihood. In generalized additive logistic models,  we consider a basis expansions of features and again assume that the log odds are related to a linear combination of $\mbf{Z}_i$. Because $\mbf{Z}_i$ is typically of high dimension, we noted the parameters are estimated using penalized maximum likelihood. In Bayesian networks, we noted that $P_{\mbf{G}_j|E}(\mbf{g}_j|e)$ is typically estimated using maximum likelihood where the continuous components are modeled using a mxiture of Gaussian densities and the discrete components are modeled non-parametrically.  Once we identify the terminal nodes in binary trees or the neighbors in k-nearest neighbors, we estimate the probability of experiencing the event using non-parametric maximum likelihood. That is we take the average among all those subjects within the same terminal node or in the given ``neighborhood'' (each terminal node and neighborhood has its own parameter). It has been well established that as the sample size tends to infinity and other regularity conditions hold (including the number of observations in terminal nodes and number of neighbors also increase), maximum likelihood consistently estimate the risk probability.

Inverse probability of censoring weighted observations ``works'' because we are able to approximate the log likelihood that we would have obtained had their not been censoring. Consider the general likelihood $\ell(\bbeta; \mathbf{X},\mathbf{E}) = \tfrac{1}{n}\sum_{i=1}^n \ell_i(\beta; E_i,\mbf{Z_i})$ for the parameter $\beta$ had no observations been right-censored. By the law of large numbers $\tfrac{1}{n}\sum_{i=1}^n \ell_i(\beta; E_i,\mbf{Z_i})$ converges $\mathcal{E} \{\ell_i(\beta; E_i,\mbf{Z_i}) \}$ where $\mathcal{E}(\cdot)$ is the expectation. In the case IPCW estimators, we maximize the likelihood  $\tfrac{1}{n}\sum_{i=1}^n \omega_i \ell_i(\beta; E_i,\mbf{Z_i})$. Note that we can rewrite $\omega_i$ as $\indicatorBig{\min(T_i,\tau)<C_i}/ \hat{G}\{\min(T_i,\tau)\}$ which approaches $\indicatorBig{\min(T_i,\tau)<C_i}/ G\{\min(T_i,\tau)\}$ for large samples. Note that
\begin{eqnarray*}
\mathcal{E} \left \{\tfrac{1}{n}\sum_{i=1}^n \omega_i \ell_i(\beta; E_i,\mbf{Z_i}) \right \} &=& \mathcal{E} \left \{  \frac{\indicatorBig{\min(T_i,\tau)<C_i}}{G\{\min(T_i,\tau)\}} \ell_i(\beta; E_i,\mbf{Z_i})  \right \} \\
&=& \mathcal{E} \left [ \mathcal{E} \left \{  \frac{\indicatorBig{\min(T_i,\tau)<C_i}}{G\{\min(T_i,\tau)\}} \ell_i(\beta; E_i,\mbf{Z_i}) | E_i, T_i, \mbf{Z}_i \right \} \right] \\
&=&  \mathcal{E} \left \{ \frac{ \mathcal{E} \{\indicatorBig{\min(T_i,\tau)<C_i} |E_i, T_i, \mbf{Z}_i \}}{G\{\min(T_i,\tau)\}} \ell_i(\beta; E_i,\mbf{Z_i}) \right \}  \\
&-& \mathcal{E} \left \{ \frac{ \mathcal{E} \{\indicatorBig{\min(T_i,\tau)<C_i} \}}{G\{\min(T_i,\tau)\}} \ell_i(\beta; E_i,\mbf{Z_i}) \right \}  \\
&=& \mathcal{E} \left \{  \frac{ \mathcal{E} \{G\{\min(T_i,\tau)\} \}}{G\{\min(T_i,\tau)\}} \ell_i(\beta; E_i,\mbf{Z_i}) \right \}  \\
&=& \mathcal{E} \left \{  \ell_i(\beta; E_i,\mbf{Z_i}) \right \}  \\
\end{eqnarray*}

That is, for large samples the IPCW log likelihood converges to the same quantity as the ``fully-observed'' likelihood. Because the difference in the IPCW and fully-observed likelihoods are (asymptotically) negligible, the IPCW approach inherits all the properties of machine learning estimators if we had full data.

\section{Example Application: Predicting cardiovascular risk using electronic health data}\label{sec:data}

We now illustrate the application of IPC-weighted risk prediction methods to the problem of predicting the risk of a cardiovascular event from electronic health data. The data come from a healthcare system in the Midwestern United States, and were extracted from the HMO Research Network Virtual Data Warehouse (HMORN VDW) associated with that system. The VDW stores data including insurance enrollment, demographics, pharmaceutical dispensing, utilization, vital signs, laboratory, census and death records. This health care system includes both an insurance plan and a medical care network in an open system which is partially overlapping. That is, patients of the insurance plan may be served by either the internal medical care network and or by external healthcare providers, and the medical care network serves patients within and outside of the insurance plan.  Patient-members who do not visit any of the clinics and hospitals in-network do not have any medical information (e.g., blood pressure information) included in the electronic medical record (EMR) of this system. Furthermore, once the patient-member disenrolls from the HMO, the patient is right-censored as there is no longer any information on risk factors or outcomes (i.e., CV events) recorded in the EMR or insurance claims data.

%%%
\subsection{Defining the study population}
\label{sect:studypop}

The study population was initially selected from those enrolled in the insurance plan between 1999 and 2011 and who had at least one outpatient medical encounter at an in-network clinic. From this initial database of 448,306 subjects, an analysis population of was identified by applying the following inclusion/exclusion criteria.

\begin{enumerate}
\item To ensure sufficient time to collect baseline risk factors on subjects, the analysis was restricted to those subjects with at least one year of continuous insurance enrollment. Some of the patients were sporadically enrolled during the period of study; however, for the purpose of our analysis, we ignored gaps in enrollment less than 90 days and considered a patient-member continuously enrolled over this period. These gaps in enrollment are likely due to administrative errors or patients changing employers but still electing coverage with the same HMO.
\item We included only patients with two medical encounters in the in-network clinic with blood pressure information at least 30 days but at most 1.5 years apart, with drug coverage, and greater than 18 years of age at the end of the baseline period.
\item Patients under the age of 40 were excluded.
\item Subjects with pre-existing serious comorbidities (prior CV event, chronic kidney disease, etc., not including diabetes) were excluded.
\end{enumerate}

These criteria ensure that the analysis population consists of a diverse group of subjects at non-trivial risk of experiencing a CV event, who were treated routinely in the primary care clinic. Patients who are only infrequently treated in the emergency room or urgent care clinics (i.e., settings where patients are unlikely to be counselled about their CV risk) were not of interest in this analysis. Subjects with comorbidities were excluded because comorbidity information was recorded inconsistently and because, as a group, these subjects have an extremely high 5-year event rate and hence are of limited interest for risk prediction models because they are almost uniformly being aggressively treated to manage their cardiovascular risk. After applying the above criteria, our final analysis dataset contained XXXX individuals.

The available longitudinal data on each patient-member were divided into: (i) a \emph{baseline} period, where the risk factors were ascertained, and (ii) a \emph{follow-up} period, where we assessed whether a patient experienced a CV event (and, if so, when).  The baseline period consisted of the time between the first blood pressure reading during the enrollment period and the date of the final blood pressure reading at most 1.5 years from the first measurement.  The follow-up period for a patient begins at the end of the baseline period and continues until either the patient experiences a CV event (defined below), the patient disenrolls from the HMO for more than 90 days, or the data capture period ends (in 2011), whichever comes first. The distribution of the follow-up periods for the resulting analysis cohort is shown in Figure~\ref{figure:followDist}, which illustrates that a large proportion of subjects are censored before five years of follow-up information is available.

\begin{figure}[h]
\centering
\includegraphics[width=0.50\textwidth]{Figure2.eps}
\caption{Distribution of follow-up times, i.e., time from the end of the baseline period until the patient experiences a CV event, the patient disenrolls from the HMO for more than 90 days, or the study ends, in our entire cohort after applying inclusion/ exclusion criteria detailed in Section \ref{sect:studypop}.}
\label{figure:followDist}
\end{figure}

%%%
\subsection{Risk factor ascertainment}
\label{sect:riskfactors}

Risk factors used in the prediction models included age, gender, systolic blood pressure (SBP), use of blood pressure medications, cholesterol markers (HDL and total cholesterol), body mass index (BMI), smoking status, and presence/absence of diabetes.  Summary statistics and brief descriptions for the risk factors are given in Table \ref{table:cohortProps}. Missing risk factor values were filled in using multiple imputation by chained equations \citep{REF} to create a dataset with no mising values; Table \ref{table:cohortProps} displays the percentage of missing values for each risk factor in the original (pre-imputation) data set. [NEED TO FILL IN MISSINGNESS \% FOR TOTAL CHOLESTEROL].

\begin{center}
\begin{table}
\tbl{Distribution of risk factors in the analysis dataset.\label{table:cohortProps}}{%
\begin{tabular}{l|ccl}\hline \noalign{\smallskip}
                          & {\bf{Median (IQR)}}       &   {\bf{\% missing}}                &                                                                          \\
{\bf{Feature Name}}       &      or                   & {\bf{(in}} &  {\bf{Description}}                                                      \\
                          & {\bf{N (\%)}}             &     {\bf{original data)}}              &                                                                          \\ \noalign{\smallskip}\hline\noalign{\smallskip} \hline
{\bf{Gender}}             &                           &                   &                                                                          \\
\ \ \ Female              & 51,530 (59.0)            &  0                &                                                                          \\
\ \ \ Male                &  35,833 (41.0)            &  0                &                                                                          \\
{\bf{Age }}(Years)        & 52      (46  -  60)       &  0                &  Age  at the end of the baseline period                                  \\
{\bf{SBP }}(mm Hg)        & 123     (115 - 133)       &  0                &  Average systolic blood pressure  during baseline period                    \\
{\bf{BMI }}(kg/m$^2$)     & 28.0    (24.7  -  32.3)   &  14               & Body mass index                                                          \\
{\bf{HDL }}(mg/dL)        & 48      (40  -  59)       &  55               &  Final high density lipoprotein during baseline period                  \\
{\bf{Total cholesterol }}(mg/dL)        & 196      (172  -  222)       &  XXX               &  Final total cholesterol during baseline period                  \\
{\bf{Smoking}}            &                           &                   & Smoking status in EMR                                                    \\
\ \ \ Never or Passive    & 64,335 (73.6)            &   0               &                                                                          \\
\ \ \ Quit                & 9,829  (11.3)             &   0               &                                                                          \\
\ \ \ Current             & 13,199  (15.1)            &   0               &                                                                          \\
{\bf{SBP Meds}}           &                           &                   &  Number of SBP medication classes filled during baseline period          \\
\ \ \ 0                   & 49,165 (56.3)            &   0               &                                                                          \\
\ \ \ 1+                & 38,198 (43.7)             &   0               &                                                                          \\
{\bf{Diabetes}}            &                           &                   & Subject has a current diagnosis of diabetes                                                    \\
\ \ \ No               & 80,921  (92.6)             &   0               &                                                                          \\
\ \ \ Yes    & 6,442 (7.4)            &   0               &                                                                          \\
\noalign{\smallskip}
\hline
\end{tabular}}
\end{table}
\end{center}


%{\textbf{Systolic blood pressure (SBP):}} Calculated as an average of all the blood pressure measurements taken during the baseline period. Blood pressure readings obtained during emergency department visits, urgent care visits, hospital admission, and during procedures (e.g., surgeries) were excluded from consideration because they may be influenced by acute conditions.
%
%{\textbf{Body mass index (BMI):}} Calculated as a function of patient's height and weight. The height of an individual is the average height measured at any encounter (possibly outside of the baseline period). Because all subjects in the analysis dataset are over 18 years of age, we expect height to remain relatively constant over the follow-up period.  The weight is calculated as an average of all weight measurements taken during the baseline period.
%
%{\textbf{Low density lipoprotein (LDL), high density lipoprotein (HDL), and triglycerides (TRG):}} The most recent laboratory measurements before the end of the baseline period is used for these lipid measures including low density lipoprotein, high density lipoprotein, and triglycerides.
%
%{\textbf{Smoking status:}} Information about smoking is complicated by the fact that many individual's responses vary considerably over time.  In our dataset, there are four categories of smoking history, never smoked, smoking, quit smoking, and passive (i.e., second-hand) smoking.  In our analysis, a person is considered to have never smoked only if they consistently recorded ``no smoking'' throughout their association with the insurance provider. A person who has recorded at least two ``smoking'' responses is considered currently smoking.  For the purpose of constructing the model we combine the ``passive smoking'' and ``no smoking'' categories.
%
%{\textbf{SBP and LDL medications:}} In our model, SBP medications are represented as the number of different medication categories a person is prescribed at the end of the baseline period. In particular, SBP medication categories included: alpha-blockers, beta-blockers, calcium-blockers, ace-inhibitors, angiotensin, vasodialator, and diuretics. LDL medications represents an indicator for whether or not a patient is taking any LDL lowering medications, such as statins and fibrates, at the end of the baseline period.  For our analysis, we ignore information regarding the specific drug dosages because it is difficult to make comparisons between doses from different variants of the same drug.

%A person is considered to be on a particular medication if a prescription for that medication is filled at most 4 months before the end of the baseline %period. We chose a time period of 4 months before the end of baseline because prescriptions are typically valid for 3 months.

%{\textbf{Comorbidities:}} Comorbidities represent serious pre-existing conditions (diseases) and previously occurred CV events or procedures (surgeries).  The existence of comorbidities significantly elevate the risk of having a CV event in the future.  In our study, we included the presence of any of the following diagnoses (including a diagnosis of a ``history of’’ these conditions)  at any point before the end of the baseline period: chronic kidney disease, coronary heart disease, cardiovascular disease, peripheral artery disease, atrial fibrillation, congestive heart failure, myocardial infarction (MI), and stroke. As we discuss below, the diagnosis may be part of the EMR or contained as part of an insurance claim. The risk prediction models that we consider treat comorbidities as a binary variable.

\subsection{Events and censoring}

Cardiovascular events ($\delta = 1$) are defined as the first recorded stroke, myocardial infarction (MI), or procedure proximal to stroke or MI (e.g., coronary artery bypass surgery, stent for either the coronary arteries or carotid artery) after the baseline period, prior to 5 years of follow-up. This information is obtained from diagnosis codes recorded by physicians or inferred from procedures (such as bypass surgery or stent placement) performed on an individual. In addition to using procedure and diagnosis codes to infer if a CV event occurred, we consider a patient to have experienced a CV event if the cause of death listed on the death certificate included MI or stroke. Subjects who do not experience an event within 5 years of the baseline period (including both those with $>5$ years follow-up who may experience an event after 5 years, and those with $< 5$ years of follow-up) are recorded as censored ($\delta =0$).

The total number of first CV events recorded within the follow-up period is 3,653; the 5-year event rate for the entire analysis cohort calculated via Kaplan-Meier is 6.4\%.

%%%
\section{Models and results} \label{sect:data-analysis}

We applied and evaluated three variants of each of the machine learning techniques described in Section \ref{sect:examples} to our data. The variants differ in their handling of censored observations:
\begin{enumerate}
\item \textbf{Treat censored observations as non-events}. Techniques using this strategy are denoted with the prefix \emph{Zero-}.
\item \textbf{Discard censored observations}. Techniques using this strategy are given the prefix \emph{Discard-}.
\item \textbf{Use IPCW}. The resulting techniques, as described in Section \ref{sect:examples}, have the prefix \emph{IPCW-}.
\end{enumerate}

Model performance was assessed based on calibration (as described in Section \ref{sec:calibration}) and discrimination metrics, i.e., the C-index and cNRI (as described in Sections \ref{sec:cIndex} and \ref{sec:NRI}). To calculate the calibration statistic and cNRI, we defined five risk strata based on clinically relevant cutoffs for the risk of experiencing a cardiovascular event within 5 years: 0-5\%, 5-10\%, $10-15\%$, $15-20\%$ and $>$ 20\%. For the cNRI, risk predictions for an individual were considered discordant between two models if the predictions fell in different ranges. With the exception of the Bayesian network, for which custom Matlab code was written, all models were fitted using the open-source statistical software program R \citep{REF}.

We now provide some implementation details for the various machine learning techniques.

\subsection{Logistic regression}

Logistic regression models were defined as
\[
\log \left( \frac{p_E}{1 - p_E} \right) = \beta_0 + \beta_1 RF_1 + \beta_2 RF_2 + \dots
\]
where $p_E$ is the probability of a CV event and $RF_1, RF_2, \dots$ are variables representing the (unscaled) risk factors described in Section \ref{sect:riskfactors}. The reported results are for models with a single ``main effect'' term for each predictor (i.e., no interactions or transformations); predictive performance did not markedly improve when second-order interaction terms were included (data not shown). Models were fitted using the \texttt{glm} function in R; IPC weights were incorporated using the \texttt{weights} argument.

\subsection{Bayesian networks}

Figure \ref{figure:BayesNet} displays the structure of the Bayesian network that we used to construct our prediction models. The structure was determined by combining known relationships from the medical literature with input from our clinical colleagues.

\begin{figure*}[h]
\centering
[ NEED TO REDRAW THIS GRAPHIC FOR THE NEW SET OF VARIABLES WE USED. ]
\includegraphics[width=0.75\textwidth]{bn_model.eps}
\caption{The graphical model for our Bayesian network for CV risk prediction.  The figure includes the structure of risk factors, conditioned on the CV event status.  In particular, nodes represent input variables and edges represent conditional dependencies between the variables.  The reader should also assume that our outcome variable (CV Event) is connected to every node in the graph (omitted in the picture for parsimony).  Continuous and discrete variables are indicated by elliptical and rectangular nodes, respectively. Nodes in boxes with rounded corners indicate that they are modelled jointly. The nodes are grouped into subgraphs indicated by the dashed boxes. The grey edge between subgraphs indicates an edge from every node in the source subgraph to every node in the destination subgraph or node.  The full description of each of the features appears in Section~\ref{sec:data}. {\em Smoke}: current smoking status of patient; {\em BMI}: body mass index of patient; {\em SBP}: systolic blood pressure; {\em SBP Med}: Number of blood pressure medication classes currently prescribed to patient; {\em HDL}: high density lipoprotein; {\em TC}: total cholesterol (note that HDL and TC are modelled jointly).}
\label{figure:BayesNet}
\end{figure*}

Nodes were jointly modeled as described in Section \ref{sect:BN}, assuming a single multivariate Normal distribution for each jointly modeled set of continuous codes ($M=1$ in Equation \eqref{eq:JoinDistComps2}). Risk factors were scaled to zero mean and unit variance prior to fitting the model, which was custom-coded in Matlab. Code is available upon request.

\subsection{Classification trees}

Classification trees were built using the \texttt{rpart} package in R, which implements the classification and regression trees described in \citet{Breiman1984}. Nodes are split based on the Gini loss criterion; the loss matrix was modified to give 10 times as much weight to incorrect non-event predictions among those experiencing events than the reverse, to induce additional splits and improve discrimination among the large fraction of the population with a relatively low (e.g., $< 5\%$) 5-year CV event risk. The minimum number of subjects in each terminal node was fixed at 200. Risk factors were not scaled prior to fitting the tree. In this illustrative example, no explict pruning was performed, though this would be advisable to optimize performance in practice. IPC weights were incorporated via the \texttt{weights} argument in \texttt{rpart}, which treats them as case weights. [ DO WE NEED TO NORMALIZE THE WEIGHTS TO SUM TO SOMETHING SPECIFIC? ]

%%%%
\section{Results} \label{sect:data-analysis2}

The full training dataset consists of 65,522 patients (75\% of the entire analysis population) drawn at random from the cohort. 52\% were censored prior to five years, so that \emph{Discard-} models were trained on 31,345 subjects. The performance of all models is evaluated based on the risk predictions of the remaining 21,841 patients not included in any training set. The calibration and discrimination of the models evaluated on the test set are summarized in Table~\ref{table:resSummary}.

\begin{table}[ht]
\centering
\tbl{Calibration and discrimination of the models described in Table~\ref{table:modelDisc}, evaluated on the hold-out test set. {\em{Predicted event rate}}: Average predicted probability of experiencing a CV event within 5 years;  {\em{Calibration}}: calibration test statistic K; {\em{C-index}}: Concordance index; {\em{cNRI}}: net reclassification improvement for censored outcomes.  \label{table:resSummary} }{%
\begin{tabular}{lcccc} \hline\noalign{\smallskip}
                    &  Predicted event rate (\%) & Calibration statistic &   C-index   & cNRI (\%)       \\
                    \noalign{\smallskip}\hline\noalign{\smallskip}
  Zero-Tree & 4.15 & 129.947 & 0.792 &  \\
  Discard-Tree & 7.10 & 66.520 & 0.793 &  \\
IPCW-Tree & 5.38 & 18.256 & 0.803 & -3.2 \\
  \hline
  Zero-Logistic & 4.14 & 115.398 & 0.809 &  \\
  Discard-Logistic & 7.08 & 49.570 & 0.809 &  \\
  IPCW-Logistic & 5.34 & 17.869 & 0.809 & -2.8 \\
  \hline
Zero-Bayes             &        6.05                &  137.1 (2.6)            &  0.8843     &     9.40        \\
Discard-Bayes             &        XXX                &  XXX            &  XXX     &     9.40        \\
IPCW-Bayes            &        4.40                &    7.3 (0.5)            &  0.8845     &     5.50        \\
\end{tabular}}
\end{table}

[ THOUGHT: COMPUTE AN AVERAGE CONDITIONAL EVENT RATE FOR COMPARISON USING THE COX MODEL? ]

It is clear from Table~\ref{table:resSummary} that both the \emph{Discard-} and \emph{Zero-} variants are poorly calibrated, yielding biased estimates of both the overall and subgroup-specific risks. The amount of improvement in reclassification performance due to using IPCW is technique-dependent; C-index is better for \emph{IPCW-Tree} than \emph{Zero-Tree} and \emph{Discard-Tree}, but there is no apparent effect for logistic regression.

\section{Discussion}

[ REWRITE ]

[ ONE PARAGRAPH: Allows use of ensemble methods ]

\subsection{Generalizability and future work}
Though motivated by an example in electronic health data, our technique is generally applicable to any situation where event outcomes are subject to censoring. For example, in economics, one might wish to predict whether recently-unemployed individuals will be re-hired within a fixed time period, an outcome which is likely to be censored in most feasible study designs. In our context, we plan to incorporate this technique into a point-of-care clinical decision support system, which will provide more accurate cardiovascular risk predictions for patients based on their individual health history.

% Acknowledgments
%\begin{acks}
%The authors would like to thank Dr. Maura Turolla of Telecom
%Italia for providing specifications about the application scenario.
%\end{acks}

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{ML_for_censored_refs}

% History dates
%\received{February 2007}{March 2009}{June 2009}

% Electronic Appendix
%\elecappendix

%\medskip
%
%\section{This is an example of Appendix section head}
%
%Channel-switching time is measured as the time length it takes for
%motes to successfully switch from one channel to another. This
%parameter impacts the maximum network throughput, because motes
%cannot receive or send any packet during this period of time, and it
%also affects the efficiency of toggle snooping in MMSN, where motes
%need to sense through channels rapidly.
%
%By repeating experiments 100 times, we get the average
%channel-switching time of Micaz motes: 24.3 $\mu$s. We then conduct
%the same experiments with different Micaz motes, as well as
%experiments with the transmitter switching from Channel 11 to other
%channels. In both scenarios, the channel-switching time does not have
%obvious changes. (In our experiments, all values are in the range of
%23.6 $\mu$s to 24.9 $\mu$s.)
%
%\section{Appendix section head}
%
%The primary consumer of energy in WSNs is idle listening. The key to
%reduce idle listening is executing low duty-cycle on nodes. Two
%primary approaches are considered in controlling duty-cycles in the
%MAC layer.

\end{document}
